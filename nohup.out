Training llama model  using 2 GPUs, 1 batch size per GPU, 64 gradient accumulation steps
ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-07-22 11:05:17,275] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-22 11:05:18,329] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
W0722 11:05:18.549000 3508318 torch/distributed/run.py:766] 
W0722 11:05:18.549000 3508318 torch/distributed/run.py:766] *****************************************
W0722 11:05:18.549000 3508318 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 11:05:18.549000 3508318 torch/distributed/run.py:766] *****************************************
[2025-07-22 11:05:19,702] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-22 11:05:19,708] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-22 11:05:21,349] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-22 11:05:21,353] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-22 11:05:23,670] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-07-22 11:05:23,674] [INFO] [comm.py:676:init_distributed] cdb=None
[2025-07-22 11:05:23,674] [INFO] [comm.py:707:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: omega8374 (omega8374-) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: omega8374 (omega8374-) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/dagr/open-instruct/venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W722 11:05:25.517627872 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/dagr/open-instruct/wandb/run-20250722_110526-eo3ejz6k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune__42__1753175123
wandb: ‚≠êÔ∏è View project at https://wandb.ai/omega8374-/open_instruct_internal
wandb: üöÄ View run at https://wandb.ai/omega8374-/open_instruct_internal/runs/eo3ejz6k
[
‚îÇ   FlatArguments(
‚îÇ   ‚îÇ   exp_name='finetune__42__1753175123',
‚îÇ   ‚îÇ   do_not_randomize_output_dir=False,
‚îÇ   ‚îÇ   model_name_or_path='/home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6',
‚îÇ   ‚îÇ   config_name=None,
‚îÇ   ‚îÇ   use_flash_attn=True,
‚îÇ   ‚îÇ   model_revision=None,
‚îÇ   ‚îÇ   additional_model_arguments={},
‚îÇ   ‚îÇ   low_cpu_mem_usage=False,
‚îÇ   ‚îÇ   dataset_name=None,
‚îÇ   ‚îÇ   dataset_mixer=None,
‚îÇ   ‚îÇ   dataset_mixer_list=[
‚îÇ   ‚îÇ   ‚îÇ   '/home/dagr/general-extraction/llama_finetune/data/synthetic',
‚îÇ   ‚îÇ   ‚îÇ   '1.0'
‚îÇ   ‚îÇ   ],
‚îÇ   ‚îÇ   dataset_mixer_list_splits=['train'],
‚îÇ   ‚îÇ   dataset_transform_fn=[
‚îÇ   ‚îÇ   ‚îÇ   'sft_tulu_tokenize_and_truncate_v1',
‚îÇ   ‚îÇ   ‚îÇ   'sft_tulu_filter_v1'
‚îÇ   ‚îÇ   ],
‚îÇ   ‚îÇ   dataset_target_columns=['input_ids', 'attention_mask', 'labels'],
‚îÇ   ‚îÇ   dataset_cache_mode='local',
‚îÇ   ‚îÇ   dataset_local_cache_dir='/home/dagr/open-instruct/local_dataset_cache',
‚îÇ   ‚îÇ   dataset_config_hash=None,
‚îÇ   ‚îÇ   dataset_skip_cache=False,
‚îÇ   ‚îÇ   dataset_mix_dir=None,
‚îÇ   ‚îÇ   dataset_config_name=None,
‚îÇ   ‚îÇ   max_train_samples=None,
‚îÇ   ‚îÇ   preprocessing_num_workers=16,
‚îÇ   ‚îÇ   max_seq_length=4096,
‚îÇ   ‚îÇ   overwrite_cache=False,
‚îÇ   ‚îÇ   clip_grad_norm=-1,
‚îÇ   ‚îÇ   gradient_accumulation_steps=64,
‚îÇ   ‚îÇ   learning_rate=0.0001,
‚îÇ   ‚îÇ   logging_steps=1,
‚îÇ   ‚îÇ   lora_rank=64,
‚îÇ   ‚îÇ   lora_alpha=16.0,
‚îÇ   ‚îÇ   lora_dropout=0.1,
‚îÇ   ‚îÇ   lr_scheduler_type='linear',
‚îÇ   ‚îÇ   num_train_epochs=2,
‚îÇ   ‚îÇ   output_dir='/home/projects2/dagr/models/llama3-synth/lora/finetune__42__1753175123',
‚îÇ   ‚îÇ   per_device_train_batch_size=1,
‚îÇ   ‚îÇ   use_lora=True,
‚îÇ   ‚îÇ   use_qlora=False,
‚îÇ   ‚îÇ   use_8bit_optimizer=False,
‚îÇ   ‚îÇ   warmup_ratio=0.03,
‚îÇ   ‚îÇ   final_lr_ratio=None,
‚îÇ   ‚îÇ   weight_decay=0.0,
‚îÇ   ‚îÇ   timeout=1800,
‚îÇ   ‚îÇ   reduce_loss='mean',
‚îÇ   ‚îÇ   resume_from_checkpoint=None,
‚îÇ   ‚îÇ   report_to=['wandb'],
‚îÇ   ‚îÇ   save_to_hub=None,
‚îÇ   ‚îÇ   gradient_checkpointing=False,
‚îÇ   ‚îÇ   use_liger_kernel=False,
‚îÇ   ‚îÇ   max_train_steps=None,
‚îÇ   ‚îÇ   seed=42,
‚îÇ   ‚îÇ   checkpointing_steps='1000',
‚îÇ   ‚îÇ   keep_last_n_checkpoints=3,
‚îÇ   ‚îÇ   fused_optimizer=True,
‚îÇ   ‚îÇ   load_balancing_loss=False,
‚îÇ   ‚îÇ   load_balancing_weight=0.5,
‚îÇ   ‚îÇ   clean_checkpoints_at_end=True,
‚îÇ   ‚îÇ   with_tracking=True,
‚îÇ   ‚îÇ   wandb_project_name='open_instruct_internal',
‚îÇ   ‚îÇ   wandb_entity=None,
‚îÇ   ‚îÇ   push_to_hub=True,
‚îÇ   ‚îÇ   hf_entity='Alphatek',
‚îÇ   ‚îÇ   hf_repo_id='Alphatek/open_instruct_dev',
‚îÇ   ‚îÇ   hf_repo_revision='finetune__42__1753175123',
‚îÇ   ‚îÇ   hf_repo_url='https://huggingface.co/Alphatek/open_instruct_dev/tree/finetune__42__1753175123',
‚îÇ   ‚îÇ   try_launch_beaker_eval_jobs=True,
‚îÇ   ‚îÇ   hf_metadata_dataset='allenai/tulu-3-evals',
‚îÇ   ‚îÇ   cache_dataset_only=False,
‚îÇ   ‚îÇ   add_seed_and_date_to_exp_name=True,
‚îÇ   ‚îÇ   try_auto_save_to_beaker=True,
‚îÇ   ‚îÇ   gs_bucket_path=None,
‚îÇ   ‚îÇ   oe_eval_tasks=None,
‚îÇ   ‚îÇ   oe_eval_max_length=4096,
‚îÇ   ‚îÇ   sync_each_batch=False,
‚îÇ   ‚îÇ   packing=False
‚îÇ   ),
‚îÇ   TokenizerConfig(
‚îÇ   ‚îÇ   tokenizer_name_or_path='/home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6',
‚îÇ   ‚îÇ   tokenizer_revision=None,
‚îÇ   ‚îÇ   trust_remote_code=False,
‚îÇ   ‚îÇ   use_fast=True,
‚îÇ   ‚îÇ   chat_template_name='tulu',
‚îÇ   ‚îÇ   add_bos=False,
‚îÇ   ‚îÇ   get_tokenizer_fn='get_tokenizer_tulu_v2_2',
‚îÇ   ‚îÇ   tokenizer_files_hash='9823dcfdc1121869029da45192238e85cf44f0b232a6d9dc20e4fe6f4242a14e,79e3e522635f3171300913bb421464a87de6222182a0570b9b2ccba2a964b2b4,6f38c73729248f6c127296386e3cdde96e254636cc58b4169d3fd32328d9a8ec,vocab.json not found',
‚îÇ   ‚îÇ   use_slow_tokenizer=True,
‚îÇ   ‚îÇ   tokenizer_name='/home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6',
‚îÇ   ‚îÇ   ground_truths_key='ground_truth',
‚îÇ   ‚îÇ   sft_messages_key='messages'
‚îÇ   )
]
/home/dagr/open-instruct/venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W722 11:05:27.912212746 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
by default, we will use the same split for all datasets
‚úÖ Found cached dataset at /home/dagr/open-instruct/local_dataset_cache/1f4fbd5a5d
by default, we will use the same split for all datasets
<|user|>
What is the value for 'settling_rate' according to this document? Answer with 
the exact numeric value or with the complete word group. If the value is not 
present, answer with 'N/A'. Document text:un number high
cas number high
The attribute 'gas pressure‚Äô has the value medium.
ec number 68.24 g/L
Id50 dermal rabbit 299.95 degrees Celsius
mercury content yes
oxidizing properties
201.11 milligrams per kilogram
amount of nitrite
54.4 mV
soil adsorption
52 J/K
oxidizer class
212 grams per mole

combustion products: medium. : suspended solids 177.09 um. : The saponification
number of chemical 296 mg per litresodium tallowate/em is strong296 
mg/I/strong..
zinc content: no. : specific target organ toxicity 185 particles/mL. : The 
chemical has
a bioaccumulation potential of 42 moles per liter..

The attribute 'tss' has the value yes.

dust concentration

12.22 kilopascal

The chemical compound has a melting point of 162 nephelometric turbidity units.
reproductive toxicity: 95 mol per litre

mist density: 40 joules per gram kelvin

conductivity: yes

: mass per volume 158 joules per kelvin

: The attribute ‚Äòexplosive limits upper‚Äô has the value yes.

biodegradability: no


<|assistant|>
N/A<|eot_id|>
loading configuration file /home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.53.2",
  "use_cache": true,
  "vocab_size": 128256
}

loading weights file /home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-07-22 11:05:28,246] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

‚úÖ Found cached dataset at /home/dagr/open-instruct/local_dataset_cache/1f4fbd5a5d
[2025-07-22 11:05:28,570] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-07-22 11:05:28,717] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 147, num_elems = 1.50B
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file /home/projects2/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 45,088,768 || all params: 1,280,919,552 || trainable%: 3.5200
trainable params: 45,088,768 || all params: 1,280,919,552 || trainable%: 3.5200
Creating dataloader
[2025-07-22 11:05:29,754] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-07-22 11:05:29,757] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.2, git-hash=unknown, git-branch=unknown
[2025-07-22 11:05:29,757] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-07-22 11:05:29,767] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=2
	 self.mp_world_size=1
	 self.seq_dp_world_size=2
	 self.sequence_parallel_size=1
***********************************************
[2025-07-22 11:05:29,768] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-07-22 11:05:29,769] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-07-22 11:05:29,770] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-07-22 11:05:29,779] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-07-22 11:05:29,779] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-07-22 11:05:29,779] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-07-22 11:05:29,779] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-07-22 11:05:30,062] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-07-22 11:05:30,063] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 4.43 GB         CA 4.97 GB         Max_CA 5 GB 
[2025-07-22 11:05:30,063] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.35 GB, percent = 58.0%
[2025-07-22 11:05:30,066] [INFO] [stage3.py:186:__init__] Reduce bucket size 4194304
[2025-07-22 11:05:30,066] [INFO] [stage3.py:187:__init__] Prefetch bucket size 3774873
[2025-07-22 11:05:30,332] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-07-22 11:05:30,333] [INFO] [utils.py:782:see_memory_usage] MA 1.49 GB         Max_MA 1.49 GB         CA 4.97 GB         Max_CA 5 GB 
[2025-07-22 11:05:30,333] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.34 GB, percent = 58.0%
Parameter Offload - Persistent parameters statistics: param_count = 33, numel = 67584
[2025-07-22 11:05:30,774] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-07-22 11:05:30,774] [INFO] [utils.py:782:see_memory_usage] MA 1.2 GB         Max_MA 1.73 GB         CA 4.97 GB         Max_CA 5 GB 
[2025-07-22 11:05:30,774] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.36 GB, percent = 58.0%
[2025-07-22 11:05:31,075] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-07-22 11:05:31,076] [INFO] [utils.py:782:see_memory_usage] MA 1.2 GB         Max_MA 1.2 GB         CA 4.97 GB         Max_CA 5 GB 
[2025-07-22 11:05:31,076] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.35 GB, percent = 58.0%
[2025-07-22 11:05:31,753] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-07-22 11:05:31,753] [INFO] [utils.py:782:see_memory_usage] MA 1.2 GB         Max_MA 1.2 GB         CA 1.22 GB         Max_CA 5 GB 
[2025-07-22 11:05:31,754] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.34 GB, percent = 58.0%
[2025-07-22 11:05:32,055] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-07-22 11:05:32,055] [INFO] [utils.py:782:see_memory_usage] MA 1.2 GB         Max_MA 1.2 GB         CA 1.22 GB         Max_CA 1 GB 
[2025-07-22 11:05:32,056] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.35 GB, percent = 58.0%
[2025-07-22 11:05:32,364] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-07-22 11:05:32,364] [INFO] [utils.py:782:see_memory_usage] MA 1.29 GB         Max_MA 1.33 GB         CA 1.35 GB         Max_CA 1 GB 
[2025-07-22 11:05:32,364] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.35 GB, percent = 58.0%
[2025-07-22 11:05:32,666] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-07-22 11:05:32,667] [INFO] [utils.py:782:see_memory_usage] MA 1.29 GB         Max_MA 1.29 GB         CA 1.35 GB         Max_CA 1 GB 
[2025-07-22 11:05:32,667] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.34 GB, percent = 58.0%
[2025-07-22 11:05:32,968] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-07-22 11:05:32,968] [INFO] [utils.py:782:see_memory_usage] MA 1.29 GB         Max_MA 1.37 GB         CA 1.43 GB         Max_CA 1 GB 
[2025-07-22 11:05:32,968] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.35 GB, percent = 58.0%
[2025-07-22 11:05:32,969] [INFO] [stage3.py:554:_setup_for_real_optimizer] optimizer state initialized
Starting from epoch 0 and step 0.
/home/dagr/open-instruct/venv/lib/python3.13/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
[2025-07-22 11:05:33,320] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-07-22 11:05:33,320] [INFO] [utils.py:782:see_memory_usage] MA 1.34 GB         Max_MA 1.34 GB         CA 1.43 GB         Max_CA 1 GB 
[2025-07-22 11:05:33,320] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.46 GB, percent = 58.2%
[2025-07-22 11:05:33,320] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-07-22 11:05:33,321] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-07-22 11:05:33,321] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-07-22 11:05:33,321] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-07-22 11:05:33,323] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-07-22 11:05:33,323] [INFO] [config.py:954:print] DeepSpeedEngine configuration:
[2025-07-22 11:05:33,323] [INFO] [config.py:958:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   amp_enabled .................. False
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   amp_params ................... False
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x71fe383316d0>
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   communication_data_type ...... None
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-07-22 11:05:33,324] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   dataloader_drop_last ......... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   disable_allgather ............ False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   dump_state ................... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   elasticity_enabled ........... False
[2025-07-22 11:05:33,325] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   global_rank .................. 0
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   grad_accum_dtype ............. None
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 64
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   gradient_clipping ............ 1.0
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   graph_harvesting ............. False
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   load_universal_checkpoint .... False
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   memory_breakdown ............. False
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   mics_shard_size .............. -1
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-07-22 11:05:33,326] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   optimizer_name ............... None
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   optimizer_params ............. None
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   pld_enabled .................. False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   pld_params ................... False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   prescale_gradients ........... False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   scheduler_name ............... None
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   scheduler_params ............. None
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   sparse_attention ............. None
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   steps_per_print .............. inf
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False
[2025-07-22 11:05:33,327] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   train_batch_size ............. 128
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   use_node_local_storage ....... False
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   weight_quantization_config ... None
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   world_size ................... 2
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  True
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=4194304 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=3774873 param_persistence_threshold=20480 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   zero_enabled ................. True
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True
[2025-07-22 11:05:33,328] [INFO] [config.py:958:print]   zero_optimization_stage ...... 3
[2025-07-22 11:05:33,328] [INFO] [config.py:944:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 4.194304e+06, 
        "stage3_prefetch_bucket_size": 3.774873e+06, 
        "stage3_param_persistence_threshold": 2.048000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 64, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
  0%|          | 0/8584 [00:00<?, ?it/s]WARNING:open_instruct.utils:Output directory exists but no checkpoint found. Starting from scratch.
Starting from epoch 0 and step 0.
/home/dagr/open-instruct/venv/lib/python3.13/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
  0%|          | 1/8584 [00:26<61:59:49, 26.00s/it]  0%|          | 2/8584 [00:49<58:48:17, 24.67s/it]  0%|          | 3/8584 [01:13<57:24:39, 24.09s/it]  0%|          | 4/8584 [01:36<56:42:44, 23.80s/it]  0%|          | 5/8584 [01:59<56:27:00, 23.69s/it]  0%|          | 6/8584 [02:23<56:13:44, 23.60s/it]  0%|          | 7/8584 [02:46<55:56:55, 23.48s/it]  0%|          | 8/8584 [03:09<55:39:27, 23.36s/it]  0%|          | 9/8584 [03:33<55:38:52, 23.36s/it]  0%|          | 10/8584 [03:56<55:28:55, 23.30s/it]  0%|          | 11/8584 [04:20<55:52:52, 23.47s/it]  0%|          | 12/8584 [04:44<56:51:50, 23.88s/it]  0%|          | 13/8584 [05:08<56:50:20, 23.87s/it]  0%|          | 14/8584 [05:32<56:23:38, 23.69s/it]  0%|          | 15/8584 [05:55<56:05:29, 23.57s/it]  0%|          | 16/8584 [06:18<55:51:43, 23.47s/it]  0%|          | 17/8584 [06:41<55:43:15, 23.41s/it]  0%|          | 18/8584 [07:05<56:03:59, 23.56s/it]  0%|          | 19/8584 [07:28<55:48:37, 23.46s/it]  0%|          | 20/8584 [07:52<55:30:32, 23.33s/it]  0%|          | 21/8584 [08:15<55:27:29, 23.32s/it]  0%|          | 22/8584 [08:39<55:48:40, 23.47s/it]  0%|          | 23/8584 [09:02<55:42:43, 23.43s/it]  0%|          | 24/8584 [09:26<56:00:25, 23.55s/it]  0%|          | 25/8584 [09:50<56:39:17, 23.83s/it]  0%|          | 26/8584 [10:15<57:07:05, 24.03s/it]  0%|          | 27/8584 [10:38<56:31:30, 23.78s/it]  0%|          | 28/8584 [11:01<55:54:35, 23.52s/it]  0%|          | 29/8584 [11:24<55:31:41, 23.37s/it]  0%|          | 30/8584 [11:47<55:29:55, 23.36s/it]  0%|          | 31/8584 [12:11<55:29:53, 23.36s/it]  0%|          | 32/8584 [12:34<55:31:14, 23.37s/it]  0%|          | 33/8584 [12:58<56:09:31, 23.64s/it]  0%|          | 34/8584 [13:24<57:20:34, 24.14s/it]  0%|          | 35/8584 [13:48<57:31:27, 24.22s/it]  0%|          | 36/8584 [14:11<56:42:20, 23.88s/it]  0%|          | 37/8584 [14:34<56:13:43, 23.68s/it]  0%|          | 38/8584 [14:58<55:58:11, 23.58s/it]  0%|          | 39/8584 [15:23<57:07:21, 24.07s/it]  0%|          | 40/8584 [15:49<58:54:26, 24.82s/it]  0%|          | 41/8584 [16:15<59:13:56, 24.96s/it]  0%|          | 42/8584 [16:42<60:52:59, 25.66s/it]  1%|          | 43/8584 [17:07<60:35:20, 25.54s/it]  1%|          | 44/8584 [17:33<60:57:58, 25.70s/it]  1%|          | 45/8584 [17:58<60:27:29, 25.49s/it]  1%|          | 46/8584 [18:24<60:50:07, 25.65s/it]  1%|          | 47/8584 [18:50<60:27:36, 25.50s/it]  1%|          | 48/8584 [19:14<59:45:30, 25.20s/it]  1%|          | 49/8584 [19:37<58:08:34, 24.52s/it]  1%|          | 50/8584 [20:00<56:59:14, 24.04s/it]  1%|          | 51/8584 [20:23<56:10:30, 23.70s/it]  1%|          | 52/8584 [20:47<56:20:11, 23.77s/it]  1%|          | 53/8584 [21:10<55:52:50, 23.58s/it]  1%|          | 54/8584 [21:33<55:23:51, 23.38s/it]  1%|          | 55/8584 [21:56<55:00:04, 23.22s/it]  1%|          | 56/8584 [22:19<55:00:33, 23.22s/it]  1%|          | 57/8584 [22:43<55:47:36, 23.56s/it]  1%|          | 58/8584 [23:07<56:02:08, 23.66s/it]  1%|          | 59/8584 [23:32<57:17:06, 24.19s/it]  1%|          | 60/8584 [23:59<59:09:23, 24.98s/it]  1%|          | 61/8584 [24:26<60:10:23, 25.42s/it]  1%|          | 62/8584 [24:53<61:31:35, 25.99s/it]  1%|          | 63/8584 [25:19<61:34:12, 26.01s/it]  1%|          | 64/8584 [25:46<62:26:37, 26.38s/it]  1%|          | 65/8584 [26:13<62:40:44, 26.49s/it]  1%|          | 66/8584 [26:41<63:30:46, 26.84s/it]  1%|          | 67/8584 [27:07<62:54:23, 26.59s/it]  1%|          | 68/8584 [27:33<62:39:25, 26.49s/it]  1%|          | 69/8584 [27:58<61:18:33, 25.92s/it]  1%|          | 70/8584 [28:22<59:57:02, 25.35s/it]  1%|          | 71/8584 [28:46<59:15:06, 25.06s/it]  1%|          | 72/8584 [29:10<58:31:40, 24.75s/it]  1%|          | 73/8584 [29:36<58:59:49, 24.95s/it]  1%|          | 74/8584 [30:01<59:04:34, 24.99s/it]  1%|          | 75/8584 [30:25<58:25:01, 24.72s/it]  1%|          | 76/8584 [30:49<58:07:08, 24.59s/it]  1%|          | 77/8584 [31:13<57:41:14, 24.41s/it]  1%|          | 78/8584 [31:38<58:01:09, 24.56s/it]  1%|          | 79/8584 [32:06<60:13:11, 25.49s/it]  1%|          | 80/8584 [32:32<61:04:42, 25.86s/it]  1%|          | 81/8584 [33:01<62:50:19, 26.60s/it]  1%|          | 82/8584 [33:28<63:38:44, 26.95s/it]  1%|          | 83/8584 [33:57<64:45:54, 27.43s/it]  1%|          | 84/8584 [34:24<64:16:37, 27.22s/it]  1%|          | 85/8584 [34:48<61:58:35, 26.25s/it]  1%|          | 86/8584 [35:12<60:19:29, 25.56s/it]  1%|          | 87/8584 [35:36<59:21:47, 25.15s/it]  1%|          | 88/8584 [36:01<59:07:16, 25.05s/it]  1%|          | 89/8584 [36:25<58:25:50, 24.76s/it]  1%|          | 90/8584 [36:49<57:46:56, 24.49s/it]  1%|          | 91/8584 [37:13<57:49:26, 24.51s/it]  1%|          | 92/8584 [37:37<57:18:35, 24.30s/it]  1%|          | 93/8584 [38:01<56:53:28, 24.12s/it]  1%|          | 94/8584 [38:25<56:46:37, 24.08s/it]  1%|          | 95/8584 [38:49<56:47:21, 24.08s/it]  1%|          | 96/8584 [39:13<56:39:56, 24.03s/it]  1%|          | 97/8584 [39:37<56:44:13, 24.07s/it]  1%|          | 98/8584 [40:01<56:44:28, 24.07s/it]  1%|          | 99/8584 [40:25<56:51:28, 24.12s/it]  1%|          | 100/8584 [40:49<56:50:03, 24.12s/it]  1%|          | 101/8584 [41:13<56:41:20, 24.06s/it]  1%|          | 102/8584 [41:37<56:42:18, 24.07s/it]  1%|          | 103/8584 [42:01<56:39:21, 24.05s/it]  1%|          | 104/8584 [42:25<56:46:18, 24.10s/it]  1%|          | 105/8584 [42:49<56:30:43, 23.99s/it]  1%|          | 106/8584 [43:13<56:28:46, 23.98s/it]  1%|          | 107/8584 [43:37<56:30:41, 24.00s/it]  1%|‚ñè         | 108/8584 [44:01<56:41:14, 24.08s/it]  1%|‚ñè         | 109/8584 [44:26<56:54:32, 24.17s/it]  1%|‚ñè         | 110/8584 [44:50<56:48:53, 24.14s/it]  1%|‚ñè         | 111/8584 [45:14<56:47:33, 24.13s/it]  1%|‚ñè         | 112/8584 [45:38<56:46:04, 24.12s/it]  1%|‚ñè         | 113/8584 [46:02<56:45:08, 24.12s/it]  1%|‚ñè         | 114/8584 [46:27<57:12:52, 24.32s/it]  1%|‚ñè         | 115/8584 [46:52<57:42:48, 24.53s/it]  1%|‚ñè         | 116/8584 [47:17<57:45:33, 24.56s/it]  1%|‚ñè         | 117/8584 [47:41<57:46:48, 24.57s/it]  1%|‚ñè         | 118/8584 [48:05<57:28:41, 24.44s/it]  1%|‚ñè         | 119/8584 [48:29<57:09:57, 24.31s/it]  1%|‚ñè         | 120/8584 [48:53<56:50:40, 24.18s/it]  1%|‚ñè         | 121/8584 [49:17<56:52:35, 24.19s/it]  1%|‚ñè         | 122/8584 [49:42<57:03:13, 24.27s/it]  1%|‚ñè         | 123/8584 [50:06<57:08:30, 24.31s/it]  1%|‚ñè         | 124/8584 [50:30<56:57:39, 24.24s/it]  1%|‚ñè         | 125/8584 [50:55<57:06:46, 24.31s/it]  1%|‚ñè         | 126/8584 [51:19<57:08:46, 24.32s/it]  1%|‚ñè         | 127/8584 [51:43<56:50:17, 24.20s/it]  1%|‚ñè         | 128/8584 [52:07<56:40:21, 24.13s/it]  2%|‚ñè         | 129/8584 [52:31<56:27:29, 24.04s/it]  2%|‚ñè         | 130/8584 [52:55<56:43:52, 24.16s/it]  2%|‚ñè         | 131/8584 [53:19<56:30:49, 24.07s/it]  2%|‚ñè         | 132/8584 [53:43<56:21:01, 24.00s/it]  2%|‚ñè         | 133/8584 [54:07<56:16:53, 23.98s/it]  2%|‚ñè         | 134/8584 [54:31<56:18:45, 23.99s/it]  2%|‚ñè         | 135/8584 [54:56<56:51:04, 24.22s/it]  2%|‚ñè         | 136/8584 [55:20<56:45:10, 24.18s/it]  2%|‚ñè         | 137/8584 [55:44<56:29:41, 24.08s/it]  2%|‚ñè         | 138/8584 [56:08<56:30:07, 24.08s/it]  2%|‚ñè         | 139/8584 [56:32<56:40:54, 24.16s/it]  2%|‚ñè         | 140/8584 [56:56<56:34:26, 24.12s/it]  2%|‚ñè         | 141/8584 [57:20<56:35:21, 24.13s/it]  2%|‚ñè         | 142/8584 [57:45<56:53:27, 24.26s/it]  2%|‚ñè         | 143/8584 [58:09<56:56:26, 24.28s/it]  2%|‚ñè         | 144/8584 [58:33<56:39:23, 24.17s/it]  2%|‚ñè         | 145/8584 [58:57<56:24:12, 24.06s/it]  2%|‚ñè         | 146/8584 [59:21<56:19:06, 24.03s/it]  2%|‚ñè         | 147/8584 [59:45<56:27:20, 24.09s/it]  2%|‚ñè         | 148/8584 [1:00:09<56:11:45, 23.98s/it]  2%|‚ñè         | 149/8584 [1:00:33<56:03:58, 23.93s/it]  2%|‚ñè         | 150/8584 [1:00:56<56:00:56, 23.91s/it]  2%|‚ñè         | 151/8584 [1:01:21<56:16:52, 24.03s/it]  2%|‚ñè         | 152/8584 [1:01:45<56:19:20, 24.05s/it]  2%|‚ñè         | 153/8584 [1:02:09<56:11:03, 23.99s/it]  2%|‚ñè         | 154/8584 [1:02:33<56:09:43, 23.98s/it]  2%|‚ñè         | 155/8584 [1:02:57<56:03:49, 23.94s/it]  2%|‚ñè         | 156/8584 [1:03:21<56:12:58, 24.01s/it]  2%|‚ñè         | 157/8584 [1:03:45<56:08:13, 23.98s/it]  2%|‚ñè         | 158/8584 [1:04:09<56:06:40, 23.97s/it]  2%|‚ñè         | 159/8584 [1:04:32<55:57:54, 23.91s/it]  2%|‚ñè         | 160/8584 [1:04:57<56:16:52, 24.05s/it]  2%|‚ñè         | 161/8584 [1:05:21<56:26:27, 24.12s/it]  2%|‚ñè         | 162/8584 [1:05:45<56:12:05, 24.02s/it]  2%|‚ñè         | 163/8584 [1:06:09<56:10:33, 24.02s/it]  2%|‚ñè         | 164/8584 [1:06:33<56:15:08, 24.05s/it]  2%|‚ñè         | 165/8584 [1:06:57<56:12:26, 24.03s/it]  2%|‚ñè         | 166/8584 [1:07:21<56:10:10, 24.02s/it]  2%|‚ñè         | 167/8584 [1:07:45<56:02:36, 23.97s/it]  2%|‚ñè         | 168/8584 [1:08:09<56:08:55, 24.02s/it]  2%|‚ñè         | 169/8584 [1:08:33<56:21:04, 24.11s/it]  2%|‚ñè         | 170/8584 [1:08:57<56:18:58, 24.10s/it]  2%|‚ñè         | 171/8584 [1:09:21<56:09:50, 24.03s/it]